{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbeb9b5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 122] Disk quota exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mZipFile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mECHR_Dataset.zip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mzip_ref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextractall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcases\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/software/lib/python3.9/zipfile.py:1642\u001b[0m, in \u001b[0;36mZipFile.extractall\u001b[0;34m(self, path, members, pwd)\u001b[0m\n\u001b[1;32m   1639\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(path)\n\u001b[1;32m   1641\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m zipinfo \u001b[38;5;129;01min\u001b[39;00m members:\n\u001b[0;32m-> 1642\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_member\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzipinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpwd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/software/lib/python3.9/zipfile.py:1697\u001b[0m, in \u001b[0;36mZipFile._extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m targetpath\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(member, pwd\u001b[38;5;241m=\u001b[39mpwd) \u001b[38;5;28;01mas\u001b[39;00m source, \\\n\u001b[1;32m   1696\u001b[0m      \u001b[38;5;28mopen\u001b[39m(targetpath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m target:\n\u001b[0;32m-> 1697\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mcopyfileobj(source, target)\n\u001b[1;32m   1699\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m targetpath\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 122] Disk quota exceeded"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('ECHR_Dataset.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82d09827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1380, 16)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os \n",
    "\n",
    "directory = 'cases/EN_dev'\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('json'):\n",
    "       file_path = (os.path.join(directory,filename))\n",
    "       with open(file_path, 'r') as f:\n",
    "            data = json.load(f)             \n",
    "              \n",
    "       temp_df = pd.json_normalize(data)\n",
    "     \n",
    "       df = pd.concat([df, temp_df], ignore_index=True)\n",
    "        \n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "#file_path = 'cases/training_set_tweets.txt'\n",
    "#df = pd.read_csv(file_path, delimiter='\\t', header=None, on_bad_lines='skip')\n",
    "\n",
    "#df_short = df.head(5000).dropna()  #iloc[:, 2]\n",
    "#df_short.columns=['a', 'b', 'Text', 'd']\n",
    "#print(df.shape)\n",
    "#print(df_short.shape)\n",
    "#print(df_short.columns)\n",
    "#print(df_short.Text)\n",
    "#print(df_short.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "926360ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         filtered_rows\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mlen\u001b[39m(filtered_rows)] \u001b[38;5;241m=\u001b[39m row\n\u001b[1;32m     14\u001b[0m     arr[index] \u001b[38;5;241m=\u001b[39m num_tokens\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m([x \u001b[38;5;28;01mfor\u001b[39;00m _,row \u001b[38;5;129;01min\u001b[39;00m filtered_rows\u001b[38;5;241m.\u001b[39miterrows() \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEXT\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m8000\u001b[39m]))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mmin\u001b[39m(arr))\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28msum\u001b[39m(arr))\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m         filtered_rows\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mlen\u001b[39m(filtered_rows)] \u001b[38;5;241m=\u001b[39m row\n\u001b[1;32m     14\u001b[0m     arr[index] \u001b[38;5;241m=\u001b[39m num_tokens\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m([x \u001b[38;5;28;01mfor\u001b[39;00m _,row \u001b[38;5;129;01min\u001b[39;00m filtered_rows\u001b[38;5;241m.\u001b[39miterrows() \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTEXT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8000\u001b[39;49m]))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mmin\u001b[39m(arr))\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28msum\u001b[39m(arr))\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tiktoken\n",
    "\n",
    "arr = np.zeros(1380)\n",
    "rows = df.iterrows()\n",
    "filtered_rows = pd.DataFrame(columns=df.columns)\n",
    "for index,row in rows:\n",
    "    encoding = tiktoken.get_encoding('cl100k_base')\n",
    "    text = \" \".join(\" \".join(row['TEXT']).split())\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    num_tokens = len(encoding.encode(text))\n",
    "    if(num_tokens < 8000):\n",
    "        filtered_rows.loc[len(filtered_rows)] = row\n",
    "    arr[index] = num_tokens\n",
    "\n",
    "\n",
    "print(min(arr))\n",
    "print(sum(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e9fdadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE OF DF: (1276, 16)\n",
      "NUMBER OF CUMULATIVE TOKENS IN DF TEXT: 3034685\n",
      "NUMBER OF CUMULATIVE TOKENS IN DF TEXT: 730180\n"
     ]
    }
   ],
   "source": [
    "print(\"SHAPE OF DF: \" + str(filtered_rows.shape))\n",
    "\n",
    "num_tokens_cum = 0\n",
    "\n",
    "for index,row in filtered_rows.iterrows():\n",
    "    encoding = tiktoken.get_encoding('cl100k_base')\n",
    "    text = \" \".join(\" \".join(row['TEXT']).split())\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    num_tokens = len(encoding.encode(text))\n",
    "    num_tokens_cum += num_tokens\n",
    "print(\"NUMBER OF CUMULATIVE TOKENS IN DF TEXT: \" + str(num_tokens_cum))\n",
    "\n",
    "df_short = filtered_rows.head(300)\n",
    "\n",
    "num_tokens_cum = 0\n",
    "for index,row in df_short.iterrows():\n",
    "    encoding = tiktoken.get_encoding('cl100k_base')\n",
    "    text = \" \".join(\" \".join(row['TEXT']).split())\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    num_tokens = len(encoding.encode(text))\n",
    "    num_tokens_cum += num_tokens\n",
    "print(\"NUMBER OF CUMULATIVE TOKENS IN DF TEXT: \" + str(num_tokens_cum))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b32737f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ITEMID', 'LANGUAGEISOCODE', 'RESPONDENT', 'BRANCH', 'DATE', 'DOCNAME',\n",
      "       'IMPORTANCE', 'CONCLUSION', 'JUDGES', 'TEXT', 'VIOLATED_ARTICLES',\n",
      "       'VIOLATED_PARAGRAPHS', 'VIOLATED_BULLETPOINTS', 'NON_VIOLATED_ARTICLES',\n",
      "       'NON_VIOLATED_PARAGRAPHS', 'NON_VIOLATED_BULLETPOINTS'],\n",
      "      dtype='object')\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df_short.columns)\n",
    "\n",
    "count_nan_rows = df_short.isna().sum(axis=1).sum()\n",
    "\n",
    "print(count_nan_rows)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff0c7b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "586\n",
      "2\n",
      "637\n",
      "3\n",
      "580\n",
      "4\n",
      "607\n",
      "5\n",
      "612\n",
      "6\n",
      "644\n",
      "7\n",
      "599\n",
      "8\n",
      "625\n",
      "9\n",
      "613\n",
      "10\n",
      "612\n",
      "11\n",
      "532\n",
      "12\n",
      "579\n",
      "13\n",
      "579\n",
      "14\n",
      "551\n",
      "15\n",
      "640\n",
      "16\n",
      "577\n",
      "17\n",
      "529\n",
      "18\n",
      "582\n",
      "19\n",
      "584\n",
      "20\n",
      "580\n",
      "21\n",
      "583\n",
      "22\n",
      "603\n",
      "23\n",
      "633\n",
      "24\n",
      "543\n",
      "25\n",
      "607\n",
      "26\n",
      "610\n",
      "27\n",
      "625\n",
      "28\n",
      "627\n",
      "29\n",
      "629\n",
      "30\n",
      "566\n",
      "31\n",
      "564\n",
      "32\n",
      "564\n",
      "33\n",
      "590\n",
      "34\n",
      "569\n",
      "35\n",
      "541\n",
      "36\n",
      "575\n",
      "37\n",
      "624\n",
      "38\n",
      "622\n",
      "39\n",
      "574\n",
      "40\n",
      "568\n",
      "41\n",
      "601\n",
      "42\n",
      "545\n",
      "43\n",
      "595\n",
      "44\n",
      "559\n",
      "45\n",
      "547\n",
      "46\n",
      "572\n",
      "47\n",
      "572\n",
      "48\n",
      "574\n",
      "49\n",
      "572\n",
      "50\n",
      "601\n",
      "51\n",
      "614\n",
      "52\n",
      "610\n",
      "53\n",
      "606\n",
      "54\n",
      "630\n",
      "55\n",
      "649\n",
      "56\n",
      "547\n",
      "57\n",
      "585\n",
      "58\n",
      "635\n",
      "59\n",
      "636\n",
      "60\n",
      "572\n",
      "61\n",
      "587\n",
      "62\n",
      "548\n",
      "63\n",
      "594\n",
      "64\n",
      "574\n",
      "65\n",
      "622\n",
      "66\n",
      "590\n",
      "67\n",
      "599\n",
      "68\n",
      "582\n",
      "69\n",
      "554\n",
      "70\n",
      "587\n",
      "71\n",
      "575\n",
      "72\n",
      "576\n",
      "73\n",
      "600\n",
      "74\n",
      "649\n",
      "75\n",
      "543\n",
      "76\n",
      "562\n",
      "77\n",
      "567\n",
      "78\n",
      "582\n",
      "79\n",
      "621\n",
      "80\n",
      "621\n",
      "81\n",
      "558\n",
      "82\n",
      "599\n",
      "83\n",
      "586\n",
      "84\n",
      "562\n",
      "85\n",
      "589\n",
      "86\n",
      "616\n",
      "87\n",
      "559\n",
      "88\n",
      "569\n",
      "89\n",
      "592\n",
      "90\n",
      "572\n",
      "91\n",
      "613\n",
      "92\n",
      "446\n",
      "93\n",
      "624\n",
      "94\n",
      "605\n",
      "95\n",
      "579\n",
      "96\n",
      "581\n",
      "97\n",
      "574\n",
      "98\n",
      "584\n",
      "99\n",
      "586\n",
      "100\n",
      "600\n",
      "101\n",
      "583\n",
      "102\n",
      "603\n",
      "103\n",
      "593\n",
      "104\n",
      "582\n",
      "105\n",
      "602\n",
      "106\n",
      "593\n",
      "107\n",
      "592\n",
      "108\n",
      "675\n",
      "109\n",
      "583\n",
      "110\n",
      "632\n",
      "111\n",
      "558\n",
      "112\n",
      "598\n",
      "113\n",
      "602\n",
      "114\n",
      "650\n",
      "115\n",
      "567\n",
      "116\n",
      "606\n",
      "117\n",
      "601\n",
      "118\n",
      "561\n",
      "119\n",
      "581\n",
      "120\n",
      "647\n",
      "121\n",
      "578\n",
      "122\n",
      "599\n",
      "123\n",
      "608\n",
      "124\n",
      "578\n",
      "125\n",
      "547\n",
      "126\n",
      "575\n",
      "127\n",
      "584\n",
      "128\n",
      "619\n",
      "129\n",
      "556\n",
      "130\n",
      "575\n",
      "131\n",
      "563\n",
      "132\n",
      "554\n",
      "133\n",
      "610\n",
      "134\n",
      "582\n",
      "135\n",
      "559\n",
      "136\n",
      "603\n",
      "137\n",
      "602\n",
      "138\n",
      "525\n",
      "139\n",
      "548\n",
      "140\n",
      "611\n",
      "141\n",
      "568\n",
      "142\n",
      "602\n",
      "143\n",
      "620\n",
      "144\n",
      "639\n",
      "145\n",
      "640\n",
      "146\n",
      "587\n",
      "147\n",
      "564\n",
      "148\n",
      "602\n",
      "149\n",
      "643\n",
      "150\n",
      "602\n",
      "151\n",
      "635\n",
      "152\n",
      "583\n",
      "153\n",
      "574\n",
      "154\n",
      "562\n",
      "155\n",
      "580\n",
      "156\n",
      "624\n",
      "157\n",
      "596\n",
      "158\n",
      "615\n",
      "159\n",
      "554\n",
      "160\n",
      "581\n",
      "161\n",
      "529\n",
      "162\n",
      "613\n",
      "163\n",
      "539\n",
      "164\n",
      "559\n",
      "165\n",
      "533\n",
      "166\n",
      "590\n",
      "167\n",
      "601\n",
      "168\n",
      "649\n",
      "169\n",
      "638\n",
      "170\n",
      "595\n",
      "171\n",
      "585\n",
      "172\n",
      "527\n",
      "173\n",
      "637\n",
      "174\n",
      "651\n",
      "175\n",
      "618\n",
      "176\n",
      "562\n",
      "177\n",
      "640\n",
      "178\n",
      "545\n",
      "179\n",
      "619\n",
      "180\n",
      "590\n",
      "181\n",
      "575\n",
      "182\n",
      "554\n",
      "183\n",
      "596\n",
      "184\n",
      "604\n",
      "185\n",
      "598\n",
      "186\n",
      "609\n",
      "187\n",
      "590\n",
      "188\n",
      "597\n",
      "189\n",
      "634\n",
      "190\n",
      "545\n",
      "191\n",
      "598\n",
      "192\n",
      "577\n",
      "193\n",
      "620\n",
      "194\n",
      "579\n",
      "195\n",
      "540\n",
      "196\n",
      "550\n",
      "197\n",
      "624\n",
      "198\n",
      "584\n",
      "199\n",
      "595\n",
      "200\n",
      "543\n",
      "201\n",
      "598\n",
      "202\n",
      "611\n",
      "203\n",
      "620\n",
      "204\n",
      "581\n",
      "205\n",
      "617\n",
      "206\n",
      "603\n",
      "207\n",
      "605\n",
      "208\n",
      "615\n",
      "209\n",
      "542\n",
      "210\n",
      "605\n",
      "211\n",
      "553\n",
      "212\n",
      "590\n",
      "213\n",
      "600\n",
      "214\n",
      "631\n",
      "215\n",
      "577\n",
      "216\n",
      "581\n",
      "217\n",
      "599\n",
      "218\n",
      "529\n",
      "219\n",
      "564\n",
      "220\n",
      "554\n",
      "221\n",
      "575\n",
      "222\n",
      "617\n",
      "223\n",
      "624\n",
      "224\n",
      "560\n",
      "225\n",
      "616\n",
      "226\n",
      "606\n",
      "227\n",
      "618\n",
      "228\n",
      "665\n",
      "229\n",
      "573\n",
      "230\n",
      "576\n",
      "231\n",
      "618\n",
      "232\n",
      "559\n",
      "233\n",
      "583\n",
      "234\n",
      "598\n",
      "235\n",
      "566\n",
      "236\n",
      "618\n",
      "237\n",
      "596\n",
      "238\n",
      "553\n",
      "239\n",
      "648\n",
      "240\n",
      "628\n",
      "241\n",
      "599\n",
      "242\n",
      "607\n",
      "243\n",
      "587\n",
      "244\n",
      "655\n",
      "245\n",
      "622\n",
      "246\n",
      "599\n",
      "247\n",
      "576\n",
      "248\n",
      "651\n",
      "249\n",
      "663\n",
      "250\n",
      "645\n",
      "251\n",
      "584\n",
      "252\n",
      "639\n",
      "253\n",
      "587\n",
      "254\n",
      "563\n",
      "255\n",
      "611\n",
      "256\n",
      "623\n",
      "257\n",
      "575\n",
      "258\n",
      "606\n",
      "259\n",
      "590\n",
      "260\n",
      "678\n",
      "261\n",
      "586\n",
      "262\n",
      "607\n",
      "263\n",
      "605\n",
      "264\n",
      "566\n",
      "265\n",
      "620\n",
      "266\n",
      "588\n",
      "267\n",
      "589\n",
      "268\n",
      "566\n",
      "269\n",
      "547\n",
      "270\n",
      "574\n",
      "271\n",
      "572\n",
      "272\n",
      "659\n",
      "273\n",
      "612\n",
      "274\n",
      "569\n",
      "275\n",
      "573\n",
      "276\n",
      "568\n",
      "277\n",
      "593\n",
      "278\n",
      "617\n",
      "279\n",
      "657\n",
      "280\n",
      "666\n",
      "281\n",
      "593\n",
      "282\n",
      "607\n",
      "283\n",
      "583\n",
      "284\n",
      "557\n",
      "285\n",
      "642\n",
      "286\n",
      "637\n",
      "287\n",
      "638\n",
      "288\n",
      "553\n",
      "289\n",
      "635\n",
      "290\n",
      "614\n",
      "291\n",
      "536\n",
      "292\n",
      "581\n",
      "293\n",
      "599\n",
      "294\n",
      "562\n",
      "295\n",
      "580\n",
      "296\n",
      "576\n",
      "297\n",
      "578\n",
      "298\n",
      "607\n",
      "299\n",
      "575\n",
      "300\n",
      "581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_615/3365657501.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_short['EMBEDDING'] = df_short['TEXT'].apply(lambda x: get_embedding(x, model))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 17)\n",
      "0      [-0.00242983247153461, -0.0018658250337466598,...\n",
      "1      [0.001230870489962399, 0.0004578838124871254, ...\n",
      "2      [-0.0035945656709372997, 0.0070950682274997234...\n",
      "3      [0.00996319018304348, 0.014701299369335175, -0...\n",
      "4      [0.002267291536554694, -0.01349721010774374, -...\n",
      "                             ...                        \n",
      "295    [0.003906595055013895, 0.0019697118550539017, ...\n",
      "296    [-0.0012068739160895348, 0.014544841833412647,...\n",
      "297    [0.0035572857595980167, -0.029464254155755043,...\n",
      "298    [0.010518673807382584, 0.0008521137060597539, ...\n",
      "299    [0.008434606716036797, -0.005519558675587177, ...\n",
      "Name: EMBEDDING, Length: 300, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from openai.embeddings_utils import get_embedding\n",
    "model='text-embedding-ada-002'\n",
    "\n",
    "openai.api_key = '**'\n",
    "i = iter(list(range(1, 10000)))\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "   print(next(i))\n",
    "   text = \" \".join(\" \".join(text).split()[:100])\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   print(len(text))\n",
    "   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n",
    "\n",
    "\n",
    "df_short['EMBEDDING'] = df_short['TEXT'].apply(lambda x: get_embedding(x, model))\n",
    "\n",
    "\n",
    "new_df = df_short.iloc[:, [0, 16]]\n",
    "print(df_short.shape)\n",
    "print(df_short['EMBEDDING'])\n",
    "new_df.to_csv('embedded_cases.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c2b4112f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 17)\n",
      "001-59587\n"
     ]
    }
   ],
   "source": [
    "print(df_short.shape)\n",
    "print(df_short['ITEMID'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "407b3a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ITEMID LANGUAGEISOCODE RESPONDENT         BRANCH  DATE  \\\n",
      "0    001-59587             ENG        FIN   GRANDCHAMBER  2001   \n",
      "1    001-69131             ENG        POL        CHAMBER  2005   \n",
      "2    001-88827             ENG        HUN  ADMISSIBILITY  2008   \n",
      "3   001-106159             ENG        MDA  ADMISSIBILITY  2011   \n",
      "4   001-115325             ENG        POL  ADMISSIBILITY  2012   \n",
      "..         ...             ...        ...            ...   ...   \n",
      "95  001-112141             ENG        FIN  ADMISSIBILITY  2012   \n",
      "96   001-57612             ENG        ITA        CHAMBER  1989   \n",
      "97  001-107139             ENG        SVN        CHAMBER  2011   \n",
      "98   001-22657             ENG        TUR  ADMISSIBILITY  2002   \n",
      "99   001-23945             ENG        SVK  ADMISSIBILITY  2004   \n",
      "\n",
      "                                 DOCNAME IMPORTANCE  \\\n",
      "0           CASE OF K. AND T. v. FINLAND          1   \n",
      "1        CASE OF J.S. AND A.S. v. POLAND          3   \n",
      "2                      JARFAS v. HUNGARY          4   \n",
      "3           CASE OF GALUSCHIN v. MOLDOVA          4   \n",
      "4                    OSTROWSKA v. POLAND          4   \n",
      "..                                   ...        ...   \n",
      "95                     A.A.S. v. FINLAND          4   \n",
      "96             CASE OF BROZICEK v. ITALY          2   \n",
      "97  CASE OF MANDIĆ AND JOVIĆ v. SLOVENIA          3   \n",
      "98                    CATIKKAS v. TURKEY          4   \n",
      "99                   FRATRIK v. SLOVAKIA          4   \n",
      "\n",
      "                                           CONCLUSION  \\\n",
      "0   Violation of Art. 8 with regard to emergency c...   \n",
      "1   Preliminary objection joined to merits (Art. 6...   \n",
      "2                                        Inadmissible   \n",
      "3                                        Inadmissible   \n",
      "4                                        Inadmissible   \n",
      "..                                                ...   \n",
      "95                                       Inadmissible   \n",
      "96  Preliminary objection rejected (non-exhaustion...   \n",
      "97  Preliminary objection joined to merits and dis...   \n",
      "98                                       Inadmissible   \n",
      "99                                       Inadmissible   \n",
      "\n",
      "                                               JUDGES  \\\n",
      "0   Gaukur Jörundsson;Luzius Wildhaber;Nicolas Bra...   \n",
      "1                                      Nicolas Bratza   \n",
      "2   András Sajó;Antonella Mularoni;Françoise Tulke...   \n",
      "3   Alvina Gyulumyan;Corneliu Bîrsan;Ineta Ziemele...   \n",
      "4   David Thór Björgvinsson;Ineta Ziemele;Krzyszto...   \n",
      "..                                                ...   \n",
      "95  David Thór Björgvinsson;George Nicolaou;Lech G...   \n",
      "96         C. Russo;J.A. Carrillo Salcedo;N. Valticos   \n",
      "97  Angelika Nußberger;Dean Spielmann;Elisabet Fur...   \n",
      "98                                     Nicolas Bratza   \n",
      "99                                     Nicolas Bratza   \n",
      "\n",
      "                                                 TEXT VIOLATED_ARTICLES  \\\n",
      "0   [11. At the beginning of the events relevant t...               [8]   \n",
      "1   [8. The applicants, Mr J.S. and Ms A.S. are a ...               [6]   \n",
      "2   [The applicant, Mr László Járfás, is a Hungari...                []   \n",
      "3   [1. The applicant was born in 1975 and lives i...                []   \n",
      "4   [1. The applicant, Ms Henryka Ostrowska, is a ...                []   \n",
      "..                                                ...               ...   \n",
      "95  [1. The applicant, Mr A.A.S., is a Finnish nat...                []   \n",
      "96  [14. Mr Georg Brozicek was born in Czechoslova...               [6]   \n",
      "97  [6. The applicants were born in 1959 and 1963 ...           [13, 3]   \n",
      "98  [The applicant, Hallo Çatıkkaş, is a Turkish n...                []   \n",
      "99  [The applicant, Peter Frátrik, is a Slovakian ...                []   \n",
      "\n",
      "   VIOLATED_PARAGRAPHS VIOLATED_BULLETPOINTS NON_VIOLATED_ARTICLES  \\\n",
      "0                   []                    []               [13, 8]   \n",
      "1                [6-1]                    []                    []   \n",
      "2                   []                    []                    []   \n",
      "3                   []                    []                    []   \n",
      "4                   []                    []                    []   \n",
      "..                 ...                   ...                   ...   \n",
      "95                  []                    []                    []   \n",
      "96          [6-1, 6-3]               [6-3-a]                    []   \n",
      "97                  []                    []                    []   \n",
      "98                  []                    []                    []   \n",
      "99                  []                    []                    []   \n",
      "\n",
      "   NON_VIOLATED_PARAGRAPHS NON_VIOLATED_BULLETPOINTS  \\\n",
      "0                       []                        []   \n",
      "1                       []                        []   \n",
      "2                       []                        []   \n",
      "3                       []                        []   \n",
      "4                       []                        []   \n",
      "..                     ...                       ...   \n",
      "95                      []                        []   \n",
      "96                      []                        []   \n",
      "97                      []                        []   \n",
      "98                      []                        []   \n",
      "99                      []                        []   \n",
      "\n",
      "                                            EMBEDDING  \n",
      "0   [0.018150050193071365, 0.009200424887239933, 0...  \n",
      "1   [-0.00242983247153461, -0.0018658250337466598,...  \n",
      "2   [0.001230870489962399, 0.0004578838124871254, ...  \n",
      "3   [-0.0036639715544879436, 0.007113008294254541,...  \n",
      "4   [0.00996319018304348, 0.014701299369335175, -0...  \n",
      "..                                                ...  \n",
      "95  [0.01218611840158701, 0.004593841265887022, 0....  \n",
      "96  [0.0033667758107185364, -0.014960458502173424,...  \n",
      "97  [0.007583621423691511, -0.00201779673807323, 0...  \n",
      "98  [-0.005136705003678799, -0.0007388641824945807...  \n",
      "99  [0.013842742890119553, -0.01467037107795477, 0...  \n",
      "\n",
      "[100 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8b22e5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "635\n",
      "This case involves a 56-year-old applicant who was the editor-in-chief of a local newspaper. On October 17, 2005, the newspaper published an article, coauthored by the applicant, titled \"Municipality in danger; authorities fail to see problem\". The article discussed the inadequate sewage system in the Iława municipality, emphasizing the public health risks and the need for extensive investments to improve it. The case is relevant to judicial matters such as freedom of the press and potential liability for the dissemination of information. No information regarding intoxication, fraud, or mental illness is mentioned.\n",
      "<class 'str'>\n",
      "598\n",
      "This case involves the first applicant, who owned three freshwater fish farms, being charged with offences against the Act on freshwater fish farms. The charges include intentionally exceeding fixed feed quotas, with the potential danger or risk to the environment, as well as enrichment for himself. The trial was initially scheduled for 9 December 1993 but was adjourned pending the outcome of another case.\n",
      "<class 'str'>\n",
      "553\n",
      "This case involves a female applicant who was born in 1947 and resides in Pabianice. She owns a semidetached house with a tailoring workshop employing around 20 individuals located in the other half of the building. On 14 September 1993, the applicant filed an application with the City Council seeking a ban on the workshop's operation or, at the very least, measures to reduce the noise it generated. On 11 June 1994, the Director of the Pabianice District Office, to which the applicant's initial application must have been transferred,\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def string_from_arr(text):\n",
    "     text = \" \".join(\" \".join(text).split()[:100])\n",
    "     return text.replace(\"\\n\", \" \")\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n",
    "\n",
    "input_sentence = \"\"\"This case revolves around a small community residing near a large industrial facility. Residents have experienced adverse health effects and witnessed environmental degradation, potentially associated with the facility's operations. The objective is to uncover the truth, identify responsible parties, and pursue justice on behalf of the affected community.\n",
    "The community, located in a tranquil countryside setting, has historically enjoyed a peaceful existence. However, since the establishment of the industrial facility a few years ago, residents have reported a rise in health issues such as respiratory ailments, skin conditions, and chronic illnesses. They have also observed the deterioration of local flora and fauna, as well as signs of contamination in nearby water sources.\n",
    "Initial investigations revealed elevated levels of pollutants in soil, air, and water samples obtained from the vicinity of the facility. These findings suggest a possible correlation between the facility's activities and the adverse effects experienced by the community.\n",
    "To gain further insights, renowned environmental scientists, toxicologists, and health experts were engaged. They conducted thorough analyses and studies to assess the extent of contamination, identify potential health risks, and determine the likely source of pollutants. Their expertise is crucial in substantiating claims and guiding legal action.\n",
    "Identifying the responsible parties is a key step in this case. While the primary accountability lies with the industrial facility due to its proximity and suspected connection to the adverse effects, it is essential to consider the involvement of third-party contributors or regulatory oversights.\n",
    "In collaboration with community advocates, affected individuals are encouraged to share their experiences and concerns. This collective effort aims to raise public awareness, foster unity, and empower the community.\n",
    "With compelling evidence, expert opinions, and testimonies from affected residents, a strong legal case is being built. The objective is to demonstrate the facility's negligence, its impact on the environment and public health, and the hardships faced by the community.\n",
    "Legal action seeks comprehensive resolutions, including monetary compensation for affected individuals, implementation of stringent environmental regulations, and sustainable remediation efforts to restore the local ecosystem.\n",
    "This case of environmental contamination underscores the importance of environmental stewardship, accountability, and protecting community well-being. The pursuit of justice aims to secure a safer and healthier future for the affected community while striving to prevent similar incidents in the future.\"\"\"\n",
    "\n",
    "input_embedding = np.array(get_embedding(input_sentence, model))\n",
    "\n",
    "df2 = pd.read_csv('embedded_cases.csv')\n",
    "\n",
    "df2['EMBEDDING'] = df2['EMBEDDING'].apply(eval).apply(np.array, dtype=float)\n",
    "\n",
    "embeddings = np.array(df2['EMBEDDING'].tolist())\n",
    "\n",
    "\n",
    "\n",
    "similarities = cosine_similarity([input_embedding], embeddings)\n",
    "\n",
    "\n",
    "\n",
    "sorted_indices = np.argsort(similarities[0])[::-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "top_10_case_ids= [df2['ITEMID'][idx] for idx in sorted_indices[:10]]\n",
    "\n",
    "df3 = pd.DataFrame(columns=['ITEMID', 'TEXT', 'SUMMARY'])   \n",
    "\n",
    "for id in top_10_case_ids[:3]:\n",
    "    \n",
    "    \n",
    "    temp_df = df_short[df_short['ITEMID'] == id].copy()\n",
    "    \n",
    "    text = string_from_arr(temp_df['TEXT'].values[0])\n",
    "    print(type(text))\n",
    "    print(len(text))\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "           {\"role\": \"system\", \"content\": \"You are a lawyer trying to write a concise summary of this case for other lawyers to read and compare to other cases.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Summarize this text with explicit mentions of judicially relevant facts included, such as intoxication, fraud, mental illness, etc.:\\n\\n\" + text},\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(completion.choices[0].message.content)\n",
    "    \n",
    "    temp_df.loc[:, 'SUMMARY'] = completion.choices[0].message.content\n",
    "    \n",
    "    df3 = pd.concat([df3, temp_df.iloc[:, [0, 9, 17]]], ignore_index=True)\n",
    "    \n",
    "\n",
    "    \n",
    "df3.to_csv('found_cases.csv', index=False)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "05e6df4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top_10_case_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m df3 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEXT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSUMMARY\u001b[39m\u001b[38;5;124m'\u001b[39m])     \n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtop_10_case_ids\u001b[49m:\n\u001b[1;32m      5\u001b[0m     temp_df \u001b[38;5;241m=\u001b[39m df_short[df_short[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mITEMID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mid\u001b[39m]\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(temp_df))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'top_10_case_ids' is not defined"
     ]
    }
   ],
   "source": [
    "df3 = pd.DataFrame(columns=['TEXT', 'SUMMARY'])     \n",
    "for id in top_10_case_ids:\n",
    "    \n",
    "    \n",
    "    temp_df = df_short[df_short['ITEMID'] == id]\n",
    "\n",
    "    print(type(temp_df))\n",
    "    print(temp_df.shape)\n",
    "    print(temp_df.columns)\n",
    "    \n",
    "    text = string_from_arr(temp_df['TEXT'].values[0])\n",
    "    #print(type(text))\n",
    "    #completion = openai.ChatCompletion.create(\n",
    "     #   model=\"gpt-3.5-turbo\",\n",
    "      #  messages=[\n",
    "       #     {\"role\": \"system\", \"content\": \"You are a lawyer trying to write a concise summary of this case for other lawyers to read\"},\n",
    "        #    {\"role\": \"user\", \"content\": \"Summarize this text:\\n\\n\" + text},\n",
    "        #]\n",
    "    #)\n",
    "    \n",
    "    #print(completion.choices[0].message)\n",
    "    \n",
    "    #temp_df['SUMMARY'] = completion.choices[0].message\n",
    "    \n",
    "    \n",
    "    #df3 = pd.concat([df3, temp_df.iloc[:, 9, 16]], ignore_index=True)\n",
    "    \n",
    "    \n",
    "#df3.to_csv('found_cases.csv', index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235e92d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
